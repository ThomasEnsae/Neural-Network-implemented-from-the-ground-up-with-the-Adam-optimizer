# Neural-Network-implemented-from-the-ground-up-with-the-Adam-optimizer

## Overview
This project involves the implementation of a neural network from scratch, using the Adam optimizer for efficient training. The goal is to demonstrate the core principles of neural networks, including forward propagation, backpropagation, and optimization, without relying on deep learning frameworks. This project serves as an educational tool for understanding the inner workings of neural networks and the Adam optimization algorithm.

## Features
-**Custom Neural Network Implementation**: Build a neural network with configurable layers, activation functions, and loss functions.
-**Adam Optimizer**: Integrates the Adam optimizer, a popular gradient-based optimization algorithm, to accelerate and stabilize the training process.
-**Forward and Backward Propagation**: Implements the essential operations of neural networks, including the forward pass for prediction and the backward pass for error correction.
-**Modular Design**: The codebase is structured in a modular way, allowing for easy extension and experimentation with different architectures and optimization techniques.
-**Educational Value**: Focuses on clear and concise code, with comments and documentation to help users understand each step of the process.

## Usage
1. **Configure the Network**: Customize the network architecture, such as the number of layers, neurons, and activation functions, in the config.py file.
2. **Train the Model**: Run the training script to train the neural network on a given dataset. The training process utilizes the Adam optimizer to adjust the network's weights.
3.**Evaluate the Model**: After training, evaluate the model's performance on a test dataset to measure accuracy and other relevant metrics.

## Contact

For any inquiries or issues, please contact:

- **Name**: SALOMON Thomas
- **Email**: [thomas.salomon@ensae.fr](mailto:your.email@example.com)
- **GitHub**: [ThomasEnsae](https://github.com/yourusername)
